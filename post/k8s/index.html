<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    
    <title>
      
      Despliegue de una aplicación en un Cluster nuevo de k8s - De Rookie a Leyenda
      
		</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link rel="stylesheet" type="text/css" href="/assets/css/normalize.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/icons.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/screen.css" />
    
    <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Lato:100,100i,300,300i,400,400i,700,700i|Source+Code+Pro:300,400,500,700" rel="stylesheet">
    

    
    <script src="https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/bigfoot/dist/bigfoot.js"></script>
    <link rel="stylesheet" type="text/css" href="/assets/bigfoot/dist/bigfoot-number.css" />
    <script type="text/javascript">
        $.bigfoot();
    </script>
    
    
</head>

    <body class="post-template">
        <header class="main-header">
	<div class="main-header-content">
		<h1 class="blog-title">
			<a href="/">
				
           De Rookie a Leyenda
				
			</a>
		</h1>
		<h2 class="blog-description">Ernesto Vázquez García</h2>
	</div>

	<div class="nav">
    
		
	</div>
</header>

        
<main class="content" role="main">
  <article class="post">
    <header class="post-header">
      
      <h2 class="post-title">Despliegue de una aplicación en un Cluster nuevo de k8s</h2>
      <section class="post-meta">
        <time class="post-date">February 29, 2020</time>
      </section>
    </header>
    <section class="post-content">
      

<p>Elige un sistema sencillo de instalación multinodo de k8s de los disponibles en cncf.io y despliega una aplicación explicando detalladamente las características.</p>

<p>Este cluster puede estar ubicado en MVs en tu propio equipo o en instancias nuevas en OpenStack.</p>

<hr />

<h1 id="k3s">K3S</h1>

<h2 id="introducción">Introducción</h2>

<p>Voy a utilizar k3s como sistema de instalación multinodo de k8s. Es una <strong>distribución certificada</strong> de Kubernetes. Para instalarla solo nos bajaremos un binario donde solo hace falta <strong>512 MB de RAM</strong> ya que es muy <strong>ligero</strong>.</p>

<p>El <strong>escenario</strong> que voy a utilizar va a ser 3 máquinas vagrant con debian y un cliente que va a ser mi propio equipo, aunque al princio lo estaba realizando tambien en una máquina vagrant.</p>

<h2 id="puertos-necesarios">Puertos necesarios</h2>

<p>Antes de empezar vamos a ver cuales son los puertos que deben estar accesibles.</p>

<pre><code>80: Para acceder a los servicios con Ingress.
443: Para acceder a los servicios con Ingress y HTTPS.
6443: Para acceder a la API de Kubernetes.
30000-40000: Para acceder a las aplicaciones con el servicio NodePort.
</code></pre>

<h2 id="desplegar-cluster-kubernetes">Desplegar cluster kubernetes</h2>

<p>Accedemos a la primera máquina y procedemos a <strong>instalar k3s</strong>.</p>

<pre><code>vagrant@maquina1:~$ sudo su -
root@maquina1:~# cd /usr/local/bin/
root@maquina1:/usr/local/bin# wget https://github.com/rancher/k3s/releases/download/v0.2.0/k3s

...

Saving to: ‘k3s’

k3s                                   100%[=======================================================================&gt;]  35.99M  5.35MB/s    in 7.4s    

2020-02-29 12:58:24 (4.84 MB/s) - ‘k3s’ saved [37735552/37735552]
</code></pre>

<p>Como se puede observar ocupa 35.99M.
A continuación vamos a darle <strong>permisos de ejecución</strong>.</p>

<pre><code>root@maquina1:/usr/local/bin# chmod +x k3s 
</code></pre>

<p>Para comenzar a <strong>desplegar nuestro cluster</strong> (el nodo master) ejecutaremos lo siguiente:</p>

<pre><code>root@maquina1:/usr/local/bin# k3s server &amp;
</code></pre>

<p>Ahora tendremos el comando k3s que nos permite <strong>gestionar el cluster</strong> y sin necesidad de autenticarnos en el cluster podemos ejecutar comandos con kubectl.</p>

<pre><code>vagrant@maquina1:~$ k3s kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
maquina1   Ready    &lt;none&gt;   88s   v1.13.4-k3s.1
</code></pre>

<p>Podemos observar que ya tendremos la <strong>maquina1</strong> dentro del cluster.</p>

<p>A continuación vamos a configurar la segunda máquina.</p>

<p>Para ello tendremos que acceder y descargarnos el <strong>binario k3s</strong> como hemos realizado previamente.</p>

<pre><code>vagrant@maquina2:~$ sudo su -
root@maquina2:~# cd /usr/local/bin/
root@maquina2:/usr/local/bin# wget https://github.com/rancher/k3s/releases/download/v0.2.0/k3s
root@maquina2:/usr/local/bin# chmod +x k3s 
</code></pre>

<p>Ahora vamos a necesitar <strong>el token de autentificación</strong> de la <strong>maquina1</strong> para que podamos introducir la segunda máquina en nuestro custer.</p>

<pre><code>root@maquina1:~# cat /var/lib/rancher/k3s/server/node-token 
K10e890d9eea043fe30e8ef86a508aef63e7a3b040f8973f30607ef734fa0958c28::node:3b293a3082963dc4d0ea71923fd7e93e
</code></pre>

<p>Ya podremos ejecutar el siguiente comando, donde pondremos <strong>la ip y el token</strong> de la primera máquina para que se conecta al cluster.</p>

<pre><code>root@maquina2:/usr/local/bin# k3s agent --server https://192.168.33.10:6443 --token K10e890d9eea043fe30e8ef86a508aef63e7a3b040f8973f30607ef734fa0958c28::node:3b293a3082963dc4d0ea71923fd7e93e
</code></pre>

<p>Se ejecuta <strong>k3s</strong> en la <strong>maquina2</strong> y si accedemos a la <strong>maquina1</strong> y le decimos al cluster que nos devuelva los nodos de nuestro cluster los saldria lo siguiente.</p>

<pre><code>vagrant@maquina1:~$ k3s kubectl get nodes
NAME       STATUS   ROLES    AGE     VERSION
maquina1   Ready    &lt;none&gt;   15m     v1.13.4-k3s.1
maquina2   Ready    &lt;none&gt;   2m32s   v1.13.4-k3s.1
</code></pre>

<p><img src="https://i.imgur.com/VMlsQMu.png" alt="" /></p>

<p>Ahora vamos a realizar los mismos pasos con la <strong>maquina3</strong>.</p>

<pre><code>vagrant@maquina3:~$ sudo su -
root@maquina3:~# cd /usr/local/bin/
root@maquina3:/usr/local/bin# wget https://github.com/rancher/k3s/releases/download/v0.2.0/k3s
root@maquina3:/usr/local/bin# chmod +x k3s 
root@maquina3:/usr/local/bin# k3s agent --server https://192.168.33.10:6443 --token K10e890d9eea043fe30e8ef86a508aef63e7a3b040f8973f30607ef734fa0958c28::node:3b293a3082963dc4d0ea71923fd7e93e
</code></pre>

<p>Ya tendremos los tres nodos en nuestro cluster</p>

<p><img src="https://i.imgur.com/J7OTCU4.png" alt="" /></p>

<p>También podemos ver los pods que tenemos ahora mismo en el sistema.</p>

<pre><code>vagrant@maquina1:~$ k3s kubectl get pods -n kube-system
NAME                             READY   STATUS      RESTARTS   AGE
coredns-7748f7f6df-psglj         1/1     Running     0          22m
helm-install-traefik-5rjj2       0/1     Completed   0          22m
svclb-traefik-699465787c-tmwhs   2/2     Running     0          22m
traefik-5468f76b59-zqgtp         1/1     Running     0          22m
</code></pre>

<p>A continuación vamos entrar en una máquina independiente, instalaremos <strong>kubectl</strong> y desde ahí vamos a trabajar en nuestro cluster.</p>

<p>Para ello vamos a entrar en nuestra máquina cliente.</p>

<pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo &quot;deb https://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubectl
</code></pre>

<p>Ahora vamos a configurar el entorno para autentificarnos en el cluster que acabamos de crear.</p>

<p>Vamos a crear un directorio para los ficheros de configuración para acceder al cluster.</p>

<pre><code>vagrant@cliente:~$ mkdir .kube
vagrant@cliente:~$ cd .kube/
vagrant@cliente:~/.kube$ 
</code></pre>

<p>Ahora copiamos el fichero <code>/etc/rancher/k3s/k3s.yaml</code> para cambiar la dirección del server.</p>

<p>En la línea de server cambiamos localhost por la ip del servidor del cluster.</p>

<p><img src="https://i.imgur.com/oSCIwAg.png" alt="" /></p>

<p>Para cargar las credenciales vamos a crear una variable de entorno que guarda el fichero.</p>

<pre><code>vagrant@cliente:~/.kube$ export KUBECONFIG=~/.kube/config 
</code></pre>

<p>Ya podremos hacer el siguiente comando para que nos devuelve los nodos de nuestro cluster.</p>

<pre><code>vagrant@cliente:~/.kube$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
maquina1   Ready    &lt;none&gt;   49m   v1.13.4-k3s.1
maquina2   Ready    &lt;none&gt;   36m   v1.13.4-k3s.1
maquina3   Ready    &lt;none&gt;   28m   v1.13.4-k3s.1
</code></pre>

<p><img src="https://i.imgur.com/CimyFqG.png" alt="" /></p>

<p>Ya estaria interactuando con nuestro cluster donde estarian los tres nodos.</p>

<h2 id="despliegue-de-nginx">Despliegue de Nginx</h2>

<p>Lo primero que vamos a hacer es instalar un contenedor con un servidor nginx, con la siguiente instrucción:</p>

<pre><code>vagrant@cliente:~$ kubectl create deploy nginx --image=nginx
deployment.apps/nginx created
</code></pre>

<p>Vamos a usar un recurso del cluster que se llama deploy (despliegue), donde indicaremos la imagen docker que se va a utilizar para crear el pod.</p>

<p>Este pod va a ejecutar un contenedor y este contenedor va a ejecutar un proceso que será un servidor nginx.</p>

<pre><code>vagrant@cliente:~$ kubectl get deploy,pod
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/nginx   1/1     1            1           95s

NAME                       READY   STATUS    RESTARTS   AGE
pod/nginx-5c7588df-6x2ff   1/1     Running   0          95s
</code></pre>

<p>Con ese comando podremos ver que ya está funcionando. Donde tendremos un deployment y un pod.</p>

<p>Vamos a crear un nuevo recurso en el cluster de kubernetes llamado servicios, donde nos va a permitir acceder a las aplicaciones que se están ejecutando en los contenedores. Vamos a utilizar los siguientes tipos:</p>

<ul>
<li><strong>NodePort</strong>: Este nos permite acceder desde el exterior.</li>
<li><strong>ClusterIP</strong>: Este también proporciona la posibilidad de acceder a la aplicación o al proceso del pod, pero internamente dentro del cluster de kubernetes, no desde el exterior.</li>
</ul>

<p>A continuación vamos a crear un servicio, donde indicaremos el nombre del despliegue, el puerto donde esta funcionando la aplicación y el tipo de servicio.</p>

<pre><code>vagrant@cliente:~$ kubectl expose deploy nginx --port=80 --type=NodePort
service/nginx exposed
</code></pre>

<pre><code>vagrant@cliente:~$ kubectl get services
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.43.0.1       &lt;none&gt;        443/TCP        3h53m
nginx        NodePort    10.43.142.205   &lt;none&gt;        80:31907/TCP   25s
</code></pre>

<p>Como podemos apreciar tenemos el servicio nginx, tipo NodePort  y nos ha mapeado el puerto 31907.</p>

<p>Ahora podremos acceder a la ip del master junto al puerto que nos ha dado.</p>

<p>A continuación vamos a realizar la escabilidad de los contendores.
Actualmente contamos con un pod con un contenedor que esta ejecutando nuestra aplicación, vamos a hacer que se ejecute en varios pods.</p>

<p>Para ello vamos a realizar el siguiente comando:</p>

<pre><code>vagrant@cliente:~$ kubectl scale --replicas=3 deploy/nginx
deployment.extensions/nginx scaled

vagrant@cliente:~$ kubectl get deploy,pod
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/nginx   3/3     3            3           3h2m

NAME                       READY   STATUS    RESTARTS   AGE
pod/nginx-5c7588df-5hcf8   1/1     Running   0          29s
pod/nginx-5c7588df-6x2ff   1/1     Running   0          3h2m
pod/nginx-5c7588df-6znqp   1/1     Running   0          29s
</code></pre>

<p>Como podemos apreciar con el segundo comando, se ha creado dos nuevos pods. Ahora se esta balanceando la carga entre los tres contenedores.</p>

<p>Con el siguiente comando podremos también ver que estos pods se están ejecutando en los distintos nodos o máquinas de nuestro cluster.</p>

<pre><code>
vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE    IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          3m1s   10.42.0.7   maquina1   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-6x2ff   1/1     Running   0          3h5m   10.42.1.2   maquina2   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-6znqp   1/1     Running   0          3m1s   10.42.2.3   maquina3   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>En la columna <strong>NODE</strong>, podemos ver que se están ejecutando cada pod en un nodo diferente, haciendo un mejor balanceo y rendimiento de nuestro cluster.</p>

<p>Vamos a crear un cuarto pod para ver donde se situaria.</p>

<pre><code>vagrant@cliente:~$ kubectl scale --replicas=4 deploy/nginx
deployment.extensions/nginx scaled

vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          5m44s   10.42.0.7   maquina1   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-6x2ff   1/1     Running   0          3h7m    10.42.1.2   maquina2   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-6znqp   1/1     Running   0          5m44s   10.42.2.3   maquina3   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-f6kq5   1/1     Running   0          10s     10.42.1.3   maquina2   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p><img src="https://i.imgur.com/In1rcrW.png" alt="" /></p>

<p>Podremos ver que este último pod se esta ejecutando en el segundo nodo <strong>(maquina2)</strong>.</p>

<p>Esta herramiento también nos proporciona <strong>tolerancia a fallo</strong>, vamos a ver un ejemplo.</p>

<p>En el caso de borrar un pod, ya sea porque lo borramos a mano o falla, se va a crear un nuevo pod.</p>

<pre><code>vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          5m44s   10.42.0.7   maquina1   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-6x2ff   1/1     Running   0          3h7m    10.42.1.2   maquina2   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-6znqp   1/1     Running   0          5m44s   10.42.2.3   maquina3   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-f6kq5   1/1     Running   0          10s     10.42.1.3   maquina2   &lt;none&gt;           &lt;none&gt;

vagrant@cliente:~$ kubectl delete pod nginx-5c7588df-6x2ff
pod &quot;nginx-5c7588df-6x2ff&quot; deleted

vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          11m     10.42.0.7   maquina1   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-6znqp   1/1     Running   0          11m     10.42.2.3   maquina3   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-f6kq5   1/1     Running   0          6m10s   10.42.1.3   maquina2   &lt;none&gt;           &lt;none&gt;
nginx-5c7588df-kh9ks   1/1     Running   0          4s      10.42.2.4   maquina3   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>Podemos ver que hemos eliminado un pod, pero el despliegue nos asegura que siempre vamos a tener los pods que hemos configurado.</p>

<h2 id="despligue-de-la-aplicación-guestbook">Despligue de la aplicación Guestbook</h2>

<p>Ya sabemos <strong>desplegar nginx en kubernetes</strong> como un <strong>escenario de pruebas</strong>, ahora vamos a implantar una <strong>aplicación real</strong>, como es el caso de <strong>Guestbook</strong>.</p>

<p>Los fichero que vamos a configurar son los siguientes. Donde pondremos en cada uno el nombre y las especificaciones que va a tener los despliegues.</p>

<p>Vamos a crear los ficheros para el despligue, el cual va a tener 3 replicas.</p>

<p>En el caso de nginx lo hemos configurado en la misma línea de ejecución del deploy, pero en este caso lo haremos de una manera más <strong>ordenada y profesional</strong> realizandolo en un fichero de configuración. Este uso es más <strong>eficiente</strong> por si tenemos que pasarlo a otro compañero y tener exactamente el mismo escenario.</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ nano frontend-deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: guestbook
  labels:
    app: guestbook
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook
        image: gcr.io/google_containers/guestbook:v3
        ports:
          - name: http-server
            containerPort: 3000
</code></pre>

<p>Para crear este despliegue pondremos lo siguiente:</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f frontend-deployment.yaml 
deployment.extensions/guestbook created
</code></pre>

<p>Ya lo tendremos creado y ahora se estan creado los pods, el despligue y las replicas.</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                             READY   STATUS    RESTARTS   AGE
pod/guestbook-5df4d55b75-dmhfr   1/1     Running   0          82s
pod/guestbook-5df4d55b75-t88z7   1/1     Running   0          82s
pod/guestbook-5df4d55b75-xcfr4   1/1     Running   0          82s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    &lt;none&gt;        443/TCP   9m11s

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook   3/3     3            3           82s

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75   3         3         3       82s
</code></pre>

<p>A continuación vamos a realizar el despligue de la base de datos redis master. Ya que vamos a realizar dos despligues uno como master y otro como esclavo.</p>

<p>Para ellos vamos a crear el siguiente fichero de configuración.</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ nano redis-master-deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
        - name: master
          image: redis
          ports:
            - name: redis-server
              containerPort: 6379
          env:
            - name: ALLOW_EMPTY_PASSWORD
              value: &quot;yes&quot;
            - name: REDIS_REPLICATION_MODE
              value: master
</code></pre>

<p>Para crear el despligue pondremos lo siguiente:</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-master-deployment.yaml 
</code></pre>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                                READY   STATUS              RESTARTS   AGE
pod/guestbook-5df4d55b75-dmhfr      1/1     Running             0          2m11s
pod/guestbook-5df4d55b75-t88z7      1/1     Running             0          2m11s
pod/guestbook-5df4d55b75-xcfr4      1/1     Running             0          2m11s
pod/redis-master-5769b46579-4jg97   0/1     ContainerCreating   0          6s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    &lt;none&gt;        443/TCP   10m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook      3/3     3            3           2m11s
deployment.apps/redis-master   0/1     1            0           6s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75      3         3         3       2m11s
replicaset.apps/redis-master-5769b46579   1         1         0       6s
</code></pre>

<p>Al igual que antes ahora vamos a configurar el redis esclavo:</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ nano redis-slave-deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
        - name: slave
          image: redis
          ports:
            - name: redis-server
              containerPort: 6379
          env:
            - name: ALLOW_EMPTY_PASSWORD
              value: &quot;yes&quot;
            - name: REDIS_REPLICATION_MODE
              value: slave
            - name: REDIS_MASTER_HOST
              value: redis-master
            - name: REDIS_MASTER_PORT_NUMBER
              value: &quot;6379&quot;
</code></pre>

<p>Lo creamos con el siguiente comando:</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-slave-deployment.yaml 
deployment.extensions/redis-slave created
</code></pre>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                                READY   STATUS              RESTARTS   AGE
pod/guestbook-5df4d55b75-dmhfr      1/1     Running             0          3m4s
pod/guestbook-5df4d55b75-t88z7      1/1     Running             0          3m4s
pod/guestbook-5df4d55b75-xcfr4      1/1     Running             0          3m4s
pod/redis-master-5769b46579-4jg97   1/1     Running             0          59s
pod/redis-slave-8648d79854-95dhs    0/1     ContainerCreating   0          4s
pod/redis-slave-8648d79854-9j59x    1/1     Running             0          4s
pod/redis-slave-8648d79854-wlck8    0/1     ContainerCreating   0          4s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    &lt;none&gt;        443/TCP   10m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook      3/3     3            3           3m4s
deployment.apps/redis-master   1/1     1            1           59s
deployment.apps/redis-slave    1/3     3            1           4s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75      3         3         3       3m4s
replicaset.apps/redis-master-5769b46579   1         1         1       59s
replicaset.apps/redis-slave-8648d79854    3         3         1       4s
</code></pre>

<p>Accedemos a la aplicación.</p>

<p><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl port-forward deployment/guestbook 3000:3000</code></p>

<p><img src="https://i.imgur.com/Afxj5UH.png" alt="" /></p>

<p>Como podemos apreciar la aplicación no funciona <strong>(Waiting for database connection&hellip;)</strong> porque la aplicación no puede conectar a la base de datos.</p>

<p>Vamos a ver como solucionarlo</p>

<h2 id="creacion-del-servicio">Creacion del servicio</h2>

<p>Ahora vamos a agregar los servicios.</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ nano redis-master-srv.yaml 

apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: redis-server
  selector:
    app: redis
    role: master
    tier: backend
  type: ClusterIP
</code></pre>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ nano redis-slave-srv.yaml

apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: redis-server
  selector:
    app: redis
    role: slave
    tier: backend
  type: ClusterIP
</code></pre>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ nano frontend-srv.yaml

apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: http-server
  selector:
    app: guestbook
    tier: frontend
</code></pre>

<p>Lo creamos con los siguientes comandos:</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-master-srv.yaml 
service/redis-master created

ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-slave-srv.yaml 
service/redis-slave created

ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f frontend-srv.yaml 
service/frontend created
</code></pre>

<p>Como podemos apreciar se han creado correctamente todos los nuevos servicios:</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl get services
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
frontend       NodePort    10.43.57.94     &lt;none&gt;        80:30403/TCP   3m9s
kubernetes     ClusterIP   10.43.0.1       &lt;none&gt;        443/TCP        62m
redis-master   ClusterIP   10.43.110.124   &lt;none&gt;        6379/TCP       3m16s
redis-slave    ClusterIP   10.43.144.103   &lt;none&gt;        6379/TCP       3m15s
</code></pre>

<p>Salida de todos:</p>

<pre><code>ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/guestbook-5df4d55b75-4fbsd      1/1     Running   0          19m
pod/guestbook-5df4d55b75-6gwb6      1/1     Running   0          19m
pod/guestbook-5df4d55b75-h28tq      1/1     Running   0          19m
pod/redis-master-5769b46579-78688   1/1     Running   0          19m
pod/redis-slave-8648d79854-fxlkl    1/1     Running   0          18m
pod/redis-slave-8648d79854-qksgr    1/1     Running   0          18m
pod/redis-slave-8648d79854-sl2fd    1/1     Running   0          18m

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/frontend       NodePort    10.43.57.94     &lt;none&gt;        80:30403/TCP   10m
service/kubernetes     ClusterIP   10.43.0.1       &lt;none&gt;        443/TCP        70m
service/redis-master   ClusterIP   10.43.110.124   &lt;none&gt;        6379/TCP       10m
service/redis-slave    ClusterIP   10.43.144.103   &lt;none&gt;        6379/TCP       10m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook      3/3     3            3           19m
deployment.apps/redis-master   1/1     1            1           19m
deployment.apps/redis-slave    3/3     3            3           18m

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75      3         3         3       19m
replicaset.apps/redis-master-5769b46579   1         1         1       19m
replicaset.apps/redis-slave-8648d79854    3         3         3       18m
</code></pre>

<p>Como se puede apreciar para la base de datos <strong>no ha mapeado ninguna IP</strong>, ya que el unico servicio que va a poder acceder a él será la propia aplicación, mientras que para guestbook si se ha mapeado el puerto.</p>

<p>Ahora si queremos acceder solamente tendremos que poner la ip junto al puerto, en este caso el 30403.</p>

<h2 id="conclusión">Conclusión</h2>

<p>De una <strong>manera muy sencilla</strong> hemos instalado un <strong>cluster de kubernetes</strong> con la <strong>herramienta k3s</strong> y hemos desplegado una aplicación de pruebas.</p>

<p>Con esta demostración, se puede observar las <strong>posibilidades que nos ofrece esta herramienta</strong>. Podremos gestionar la vida de nuestros contenedores e implantar de una manera más <strong>eficiente</strong> las aplicaciones o demás proyectos.</p>

<p>También una de las <strong>ventajas o desventaja</strong> que tiene es que es un proyecto <strong>muy vivo</strong> y tenemos muchas actualizaciones que nos ofrecen <strong>nuevas funcionalidades.</strong></p>

<p>Otra de las grandes <strong>ventajas de Kubernetes</strong> es poder <strong>gestionar</strong> toda nuestra infraestructura desde sus <strong>API’s</strong>.</p>

    </section>
    <footer class="post-footer">
      
    </footer>
  </article>
</main>

        <footer class="site-footer">
  <section class="rss"><a class="subscribe-button icon-feed" href="/index.xml"></a></section>
  <section class="twitter"><a class="icon-twitter" href="https://twitter.com/ernestovazgar"> ernestovazgar</a></section>
  
  <section class="copyright">&copy; 2020 De Rookie a Leyenda</section>
  <section class="poweredby"><a href="https://github.com/nirocfz/arabica">Arabica</a> theme by Sean Lunsford. Published with <a href="https://gohugo.io">Hugo</a>.</section>
</footer>



    </body>
</html>
